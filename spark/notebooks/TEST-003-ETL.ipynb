{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a21724bb-c821-4427-8c8b-6f4e86073332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType,StringType, FloatType\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a875d644-1526-4ed0-84d8-6abf13b99d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"Read_Parquet_File\")\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,\"\n",
    "            \"software.amazon.awssdk:bundle:2.24.8,\"\n",
    "            \"software.amazon.awssdk:url-connection-client:2.24.8,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.375\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .config(\"spark.sql.catalog.lakehouse_prod_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.lakehouse_prod_catalog.type\", \"nessie\")  # <--- use nessie, not rest\n",
    "    .config(\"spark.sql.catalog.lakehouse_prod_catalog.uri\", \"http://nessie:19120/api/v2\")  # <--- correct URL\n",
    "    .config(\"spark.sql.catalog.lakehouse_prod_catalog.warehouse\", \"s3://warehouse/\")\n",
    "    .config(\"spark.sql.catalog.lakehouse_prod_catalog.ref\", \"main\")  # <--- default Nessie branch\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\")   # MinIO endpoint\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\")            # MinIO access key\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"password\")            # MinIO secret key\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")           # important for MinIO\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd94368e-db30-4abc-850a-4b1fd69bae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"Read_Parquet_File\")\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \",\".join([\n",
    "                \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0\",\n",
    "                \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "                \"com.amazonaws:aws-java-sdk-bundle:1.12.375\"\n",
    "            ]))\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"password\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c1c7a0e-40d9-489d-ad0e-134cad9e9cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_csv_from_lake_s3(bucket: str, folder: str, filename: str):\n",
    "    \"\"\"\n",
    "    Read a CSV file from MinIO (S3-compatible storage) into a Spark DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    path = f\"s3a://{bucket}/{folder}/{filename}\"\n",
    "\n",
    "    logging.info(f\"Reading file from MinIO using Spark: {path}\")\n",
    "\n",
    "    try:\n",
    "        df = (\n",
    "            spark.read\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .csv(path)\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Successfully extracted {filename} with {df.count()} rows.\")\n",
    "        df.show(5, truncate=False)\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading from MinIO: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e809fea6-c624-46ba-ab07-f96523d28bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/11 17:02:55 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://landing-data/HR-LAKE-DATASET/HRIS_EMPLOYEE_20250810.csv.\n",
      "java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n",
      "\t... 26 more\n",
      "ERROR:root:Error reading from MinIO: An error occurred while calling o202.csv.\n",
      ": java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n",
      "\tat scala.collection.immutable.List.map(List.scala:293)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n",
      "\t... 29 more\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o202.csv.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 29 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m folder= \u001b[33m'\u001b[39m\u001b[33mHR-LAKE-DATASET\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m filename= \u001b[33m'\u001b[39m\u001b[33mHRIS_EMPLOYEE_20250810.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df = \u001b[43mextract_csv_from_lake_s3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m df\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mextract_csv_from_lake_s3\u001b[39m\u001b[34m(bucket, folder, filename)\u001b[39m\n\u001b[32m      8\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReading file from MinIO using Spark: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     11\u001b[39m     df = (\n\u001b[32m     12\u001b[39m         \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mheader\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minferSchema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     )\n\u001b[32m     18\u001b[39m     logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSuccessfully extracted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m     df.show(\u001b[32m5\u001b[39m, truncate=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/sql/readwriter.py:740\u001b[39m, in \u001b[36mDataFrameReader.csv\u001b[39m\u001b[34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) == \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    739\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._spark._sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonUtils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    741\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[32m    743\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunc\u001b[39m(iterator):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o202.csv.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 29 more\n"
     ]
    }
   ],
   "source": [
    "bucket= 'landing-data'\n",
    "folder= 'HR-LAKE-DATASET'\n",
    "filename= 'HRIS_EMPLOYEE_20250810.csv'\n",
    "\n",
    "df = extract_csv_from_lake_s3(bucket, folder, filename)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0514fff-4058-4309-a984-4b846e1623dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.40.50-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore<1.41.0,>=1.40.50 (from boto3)\n",
      "  Downloading botocore-1.40.50-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3)\n",
      "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/site-packages (from botocore<1.41.0,>=1.40.50->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/site-packages (from botocore<1.41.0,>=1.40.50->boto3) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.50->boto3) (1.17.0)\n",
      "Downloading boto3-1.40.50-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.40.50-py3-none-any.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.40.50 botocore-1.40.50 jmespath-1.0.1 s3transfer-0.14.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587299bc-d5e0-481c-8201-da72c3382a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ae18042-c4af-467c-a99f-36e96e3820b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>LastName</th>\n",
       "      <th>StartDate</th>\n",
       "      <th>ExitDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Supervisor</th>\n",
       "      <th>ADEmail</th>\n",
       "      <th>BusinessUnit</th>\n",
       "      <th>EmployeeStatus</th>\n",
       "      <th>...</th>\n",
       "      <th>Satisfaction Score</th>\n",
       "      <th>Work-Life Balance Score</th>\n",
       "      <th>Training Date</th>\n",
       "      <th>Training Program Name</th>\n",
       "      <th>Training Type</th>\n",
       "      <th>Training Outcome</th>\n",
       "      <th>Location</th>\n",
       "      <th>Trainer</th>\n",
       "      <th>Training Duration(Days)</th>\n",
       "      <th>Training Cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Uriah</td>\n",
       "      <td>Bridges</td>\n",
       "      <td>20-Sep-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Production Technician I</td>\n",
       "      <td>Peter Oneill</td>\n",
       "      <td>uriah.bridges@bilearner.com</td>\n",
       "      <td>CCDR</td>\n",
       "      <td>Active</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>15-Jul-23</td>\n",
       "      <td>Leadership Development</td>\n",
       "      <td>Internal</td>\n",
       "      <td>Failed</td>\n",
       "      <td>South Marisa</td>\n",
       "      <td>Taylor Rodriguez</td>\n",
       "      <td>2</td>\n",
       "      <td>606.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Paula</td>\n",
       "      <td>Small</td>\n",
       "      <td>11-Feb-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Production Technician I</td>\n",
       "      <td>Renee Mccormick</td>\n",
       "      <td>paula.small@bilearner.com</td>\n",
       "      <td>EW</td>\n",
       "      <td>Active</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>12-Sep-22</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>External</td>\n",
       "      <td>Incomplete</td>\n",
       "      <td>Tammieville</td>\n",
       "      <td>Kelly Patterson DDS</td>\n",
       "      <td>4</td>\n",
       "      <td>673.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Edward</td>\n",
       "      <td>Buck</td>\n",
       "      <td>10-Dec-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Area Sales Manager</td>\n",
       "      <td>Crystal Walker</td>\n",
       "      <td>edward.buck@bilearner.com</td>\n",
       "      <td>PL</td>\n",
       "      <td>Active</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13-Aug-22</td>\n",
       "      <td>Leadership Development</td>\n",
       "      <td>External</td>\n",
       "      <td>Failed</td>\n",
       "      <td>East Roberthaven</td>\n",
       "      <td>Taylor Thomas</td>\n",
       "      <td>2</td>\n",
       "      <td>413.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Michael</td>\n",
       "      <td>Riordan</td>\n",
       "      <td>21-Jun-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Area Sales Manager</td>\n",
       "      <td>Rebekah Wright</td>\n",
       "      <td>michael.riordan@bilearner.com</td>\n",
       "      <td>CCDR</td>\n",
       "      <td>Active</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>15-Dec-22</td>\n",
       "      <td>Project Management</td>\n",
       "      <td>External</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Garzatown</td>\n",
       "      <td>Holly Elliott</td>\n",
       "      <td>3</td>\n",
       "      <td>663.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Jasmine</td>\n",
       "      <td>Onque</td>\n",
       "      <td>29-Jun-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Area Sales Manager</td>\n",
       "      <td>Jason Kim</td>\n",
       "      <td>jasmine.onque@bilearner.com</td>\n",
       "      <td>TNS</td>\n",
       "      <td>Active</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>13-Jul-23</td>\n",
       "      <td>Technical Skills</td>\n",
       "      <td>External</td>\n",
       "      <td>Failed</td>\n",
       "      <td>Lake Meganville</td>\n",
       "      <td>Donald Martinez</td>\n",
       "      <td>5</td>\n",
       "      <td>399.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 FirstName LastName  StartDate ExitDate                    Title  \\\n",
       "0           0     Uriah  Bridges  20-Sep-19      NaN  Production Technician I   \n",
       "1           1     Paula    Small  11-Feb-23      NaN  Production Technician I   \n",
       "2           2    Edward     Buck  10-Dec-18      NaN       Area Sales Manager   \n",
       "3           3   Michael  Riordan  21-Jun-21      NaN       Area Sales Manager   \n",
       "4           4   Jasmine    Onque  29-Jun-19      NaN       Area Sales Manager   \n",
       "\n",
       "        Supervisor                        ADEmail BusinessUnit EmployeeStatus  \\\n",
       "0     Peter Oneill    uriah.bridges@bilearner.com         CCDR         Active   \n",
       "1  Renee Mccormick      paula.small@bilearner.com           EW         Active   \n",
       "2   Crystal Walker      edward.buck@bilearner.com           PL         Active   \n",
       "3   Rebekah Wright  michael.riordan@bilearner.com         CCDR         Active   \n",
       "4        Jason Kim    jasmine.onque@bilearner.com          TNS         Active   \n",
       "\n",
       "   ... Satisfaction Score Work-Life Balance Score Training Date  \\\n",
       "0  ...                  2                       3     15-Jul-23   \n",
       "1  ...                  1                       5     12-Sep-22   \n",
       "2  ...                  2                       1     13-Aug-22   \n",
       "3  ...                  5                       4     15-Dec-22   \n",
       "4  ...                  5                       3     13-Jul-23   \n",
       "\n",
       "    Training Program Name Training Type Training Outcome          Location  \\\n",
       "0  Leadership Development      Internal           Failed      South Marisa   \n",
       "1        Customer Service      External       Incomplete       Tammieville   \n",
       "2  Leadership Development      External           Failed  East Roberthaven   \n",
       "3      Project Management      External        Completed         Garzatown   \n",
       "4        Technical Skills      External           Failed   Lake Meganville   \n",
       "\n",
       "               Trainer Training Duration(Days) Training Cost  \n",
       "0     Taylor Rodriguez                       2        606.11  \n",
       "1  Kelly Patterson DDS                       4        673.02  \n",
       "2        Taylor Thomas                       2        413.28  \n",
       "3        Holly Elliott                       3        663.78  \n",
       "4      Donald Martinez                       5        399.03  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# S3 Credential\n",
    "aws_access_key_id = \"admin\"\n",
    "aws_secret_access_key = \"password\"\n",
    "\n",
    "# Create S3 Client for MinIO\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    endpoint_url=\"http://minio:9000\",  # MinIO endpoint\n",
    "    use_ssl=False\n",
    ")\n",
    "\n",
    "# Bucket Name\n",
    "bucket_name = \"landing-data\"\n",
    "\n",
    "# List objects in bucket\n",
    "response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "objects_list = response.get(\"Contents\", [])\n",
    "\n",
    "# Loop through objects and read CSV into Pandas DataFrame\n",
    "dfs = []  # list to store DataFrames\n",
    "for obj in objects_list:\n",
    "    obj_name = obj[\"Key\"]\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=obj_name)\n",
    "    object_content = response[\"Body\"].read().decode(\"utf-8\")\n",
    "\n",
    "    # Convert CSV text to Pandas DataFrame\n",
    "    df = pd.read_csv(StringIO(object_content))\n",
    "    dfs.append(df)\n",
    "\n",
    "# Combine all files into a single DataFrame (optional)\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b81ffff1-9f40-45a7-b4e7-b1388fa8f99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>LastName</th>\n",
       "      <th>StartDate</th>\n",
       "      <th>ExitDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Supervisor</th>\n",
       "      <th>ADEmail</th>\n",
       "      <th>BusinessUnit</th>\n",
       "      <th>EmployeeStatus</th>\n",
       "      <th>...</th>\n",
       "      <th>Satisfaction Score</th>\n",
       "      <th>Work-Life Balance Score</th>\n",
       "      <th>Training Date</th>\n",
       "      <th>Training Program Name</th>\n",
       "      <th>Training Type</th>\n",
       "      <th>Training Outcome</th>\n",
       "      <th>Location</th>\n",
       "      <th>Trainer</th>\n",
       "      <th>Training Duration(Days)</th>\n",
       "      <th>Training Cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Uriah</td>\n",
       "      <td>Bridges</td>\n",
       "      <td>20-Sep-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Production Technician I</td>\n",
       "      <td>Peter Oneill</td>\n",
       "      <td>uriah.bridges@bilearner.com</td>\n",
       "      <td>CCDR</td>\n",
       "      <td>Active</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>15-Jul-23</td>\n",
       "      <td>Leadership Development</td>\n",
       "      <td>Internal</td>\n",
       "      <td>Failed</td>\n",
       "      <td>South Marisa</td>\n",
       "      <td>Taylor Rodriguez</td>\n",
       "      <td>2</td>\n",
       "      <td>606.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Paula</td>\n",
       "      <td>Small</td>\n",
       "      <td>11-Feb-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Production Technician I</td>\n",
       "      <td>Renee Mccormick</td>\n",
       "      <td>paula.small@bilearner.com</td>\n",
       "      <td>EW</td>\n",
       "      <td>Active</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>12-Sep-22</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>External</td>\n",
       "      <td>Incomplete</td>\n",
       "      <td>Tammieville</td>\n",
       "      <td>Kelly Patterson DDS</td>\n",
       "      <td>4</td>\n",
       "      <td>673.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Edward</td>\n",
       "      <td>Buck</td>\n",
       "      <td>10-Dec-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Area Sales Manager</td>\n",
       "      <td>Crystal Walker</td>\n",
       "      <td>edward.buck@bilearner.com</td>\n",
       "      <td>PL</td>\n",
       "      <td>Active</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13-Aug-22</td>\n",
       "      <td>Leadership Development</td>\n",
       "      <td>External</td>\n",
       "      <td>Failed</td>\n",
       "      <td>East Roberthaven</td>\n",
       "      <td>Taylor Thomas</td>\n",
       "      <td>2</td>\n",
       "      <td>413.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Michael</td>\n",
       "      <td>Riordan</td>\n",
       "      <td>21-Jun-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Area Sales Manager</td>\n",
       "      <td>Rebekah Wright</td>\n",
       "      <td>michael.riordan@bilearner.com</td>\n",
       "      <td>CCDR</td>\n",
       "      <td>Active</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>15-Dec-22</td>\n",
       "      <td>Project Management</td>\n",
       "      <td>External</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Garzatown</td>\n",
       "      <td>Holly Elliott</td>\n",
       "      <td>3</td>\n",
       "      <td>663.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Jasmine</td>\n",
       "      <td>Onque</td>\n",
       "      <td>29-Jun-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Area Sales Manager</td>\n",
       "      <td>Jason Kim</td>\n",
       "      <td>jasmine.onque@bilearner.com</td>\n",
       "      <td>TNS</td>\n",
       "      <td>Active</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>13-Jul-23</td>\n",
       "      <td>Technical Skills</td>\n",
       "      <td>External</td>\n",
       "      <td>Failed</td>\n",
       "      <td>Lake Meganville</td>\n",
       "      <td>Donald Martinez</td>\n",
       "      <td>5</td>\n",
       "      <td>399.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 FirstName LastName  StartDate ExitDate                    Title  \\\n",
       "0           0     Uriah  Bridges  20-Sep-19      NaN  Production Technician I   \n",
       "1           1     Paula    Small  11-Feb-23      NaN  Production Technician I   \n",
       "2           2    Edward     Buck  10-Dec-18      NaN       Area Sales Manager   \n",
       "3           3   Michael  Riordan  21-Jun-21      NaN       Area Sales Manager   \n",
       "4           4   Jasmine    Onque  29-Jun-19      NaN       Area Sales Manager   \n",
       "\n",
       "        Supervisor                        ADEmail BusinessUnit EmployeeStatus  \\\n",
       "0     Peter Oneill    uriah.bridges@bilearner.com         CCDR         Active   \n",
       "1  Renee Mccormick      paula.small@bilearner.com           EW         Active   \n",
       "2   Crystal Walker      edward.buck@bilearner.com           PL         Active   \n",
       "3   Rebekah Wright  michael.riordan@bilearner.com         CCDR         Active   \n",
       "4        Jason Kim    jasmine.onque@bilearner.com          TNS         Active   \n",
       "\n",
       "   ... Satisfaction Score Work-Life Balance Score Training Date  \\\n",
       "0  ...                  2                       3     15-Jul-23   \n",
       "1  ...                  1                       5     12-Sep-22   \n",
       "2  ...                  2                       1     13-Aug-22   \n",
       "3  ...                  5                       4     15-Dec-22   \n",
       "4  ...                  5                       3     13-Jul-23   \n",
       "\n",
       "    Training Program Name Training Type Training Outcome          Location  \\\n",
       "0  Leadership Development      Internal           Failed      South Marisa   \n",
       "1        Customer Service      External       Incomplete       Tammieville   \n",
       "2  Leadership Development      External           Failed  East Roberthaven   \n",
       "3      Project Management      External        Completed         Garzatown   \n",
       "4        Technical Skills      External           Failed   Lake Meganville   \n",
       "\n",
       "               Trainer Training Duration(Days) Training Cost  \n",
       "0     Taylor Rodriguez                       2        606.11  \n",
       "1  Kelly Patterson DDS                       4        673.02  \n",
       "2        Taylor Thomas                       2        413.28  \n",
       "3        Holly Elliott                       3        663.78  \n",
       "4      Donald Martinez                       5        399.03  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "aws_access_key_id = \"admin\"\n",
    "aws_secret_access_key = \"password\"\n",
    "\n",
    "bucket = \"landing-data\"\n",
    "folder = \"HR-LAKE-DATASET\"\n",
    "file = \"HRIS_EMPLOYEE_20250810.csv\"\n",
    "\n",
    "key = f\"{folder}/{file}\"\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    endpoint_url=\"http://minio:9000\",\n",
    "    use_ssl=False\n",
    ")\n",
    "\n",
    "response = s3.get_object(Bucket=bucket, Key=key)\n",
    "data = response[\"Body\"].read().decode(\"utf-8\")\n",
    "\n",
    "df = pd.read_csv(StringIO(data))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43414b16-879f-465f-9993-f02e04ecba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac3119d8-dba6-46d0-bedb-fa8b97b8edf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-11 17:17:46\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "f = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "856cf74a-cbdf-43f3-bdbb-34109c08e8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-12 00:22:12\n",
      "Timezone Name (from tzinfo): +07\n",
      "Timezone Offset (from utcoffset): 7:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# The standard IANA timezone identifier for Thailand\n",
    "thai_timezone_name = 'Asia/Bangkok'\n",
    "thai_tz = pytz.timezone(thai_timezone_name)\n",
    "\n",
    "# Get the current time in the specified timezone\n",
    "now_thai_aware = datetime.now(thai_tz)\n",
    "\n",
    "# Format the datetime object, including the timezone name (%Z)\n",
    "# Note: pytz timezones often return the full identifier (e.g., 'Asia/Bangkok') for %Z, \n",
    "# or a standard abbreviation like 'ICT' depending on the Python version/system configuration.\n",
    "f_thai = now_thai_aware.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f_thai)\n",
    "\n",
    "# You can also get the timezone name/abbreviation directly from the datetime object:\n",
    "print(f\"Timezone Name (from tzinfo): {now_thai_aware.tzname()}\") \n",
    "print(f\"Timezone Offset (from utcoffset): {now_thai_aware.utcoffset()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca60f14e-f210-4770-a43f-233e74594af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current date and time: 2025-10-11 17:18:22.633017\n",
      "Year: 2025\n",
      "Month: 10\n",
      "Day: 11\n",
      "Hour: 17\n",
      "Minute: 18\n",
      "Second: 22\n"
     ]
    }
   ],
   "source": [
    "# Import the datetime class from datetime module\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Display the complete datetime object\n",
    "print(\"Current date and time:\", current_datetime)\n",
    "\n",
    "# Access individual components\n",
    "print(\"Year:\", current_datetime.year)\n",
    "print(\"Month:\", current_datetime.month)\n",
    "print(\"Day:\", current_datetime.day)\n",
    "print(\"Hour:\", current_datetime.hour)\n",
    "print(\"Minute:\", current_datetime.minute)\n",
    "print(\"Second:\", current_datetime.second)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f18ac84-c8fd-45b3-9bd5-2d199f46bb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
